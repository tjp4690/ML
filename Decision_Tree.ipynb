{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# *Question 1: What is a Decision Tree, and how does it work in the context of classification*\n",
        "-  Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. In the context of classification, it is used to predict a class label by learning simple decision rules inferred from the features of the data.\n",
        "\n",
        "\n",
        " How it Works:\n",
        "Structure:\n",
        "\n",
        "The tree consists of:\n",
        "\n",
        "- Root Node: Represents the entire dataset and starts the splitting process.\n",
        "\n",
        "Internal Nodes: Represent tests or decisions based on feature values.\n",
        "\n",
        "Leaf Nodes: Represent class labels or final decisions.\n",
        "\n",
        "- Splitting:\n",
        "\n",
        "The tree splits the dataset based on feature values using a criterion such as:\n",
        "\n",
        "Gini Index\n",
        "\n",
        "Entropy/Information Gain\n",
        "\n",
        "Chi-Square\n",
        "\n",
        "- Decision Path:\n",
        "\n",
        "A new data point follows a path from the root node to a leaf node based on the decisions at each node.\n",
        "\n",
        "The leaf node gives the predicted class label."
      ],
      "metadata": {
        "id": "0Cr2MBUPqUkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *** Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?***\n",
        "\n",
        "📌-  What are Impurity Measures?\n",
        "In a Decision Tree, impurity measures are used to decide how to split the data at each node. They help determine how \"pure\" or \"mixed\" a node is — i.e., whether the samples in a node mostly belong to one class or multiple classes.\n",
        "\n",
        "1️⃣ Gini Impurity\n",
        "📘 Definition:\n",
        "Gini Impurity measures the probability that a randomly chosen sample would be incorrectly classified if it were labeled according to the class distribution in that node.\n",
        "\n",
        "🔢 Formula:\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  = probability of class\n",
        "𝑖\n",
        "i at a node\n",
        "\n",
        "𝑛\n",
        "n = total number of classes\n",
        "\n",
        "🔍 Interpretation:\n",
        "Gini = 0 → Node is pure (only one class)\n",
        "\n",
        "Gini increases as classes are more mixed\n",
        "\n",
        "2️⃣ Entropy (Information Gain)\n",
        "📘 Definition:\n",
        "Entropy measures the amount of uncertainty or disorder in the data. It comes from information theory.\n",
        "\n",
        "🔢 Formula:\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑝\n",
        "𝑖\n",
        "⋅\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Entropy=−\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " ⋅log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )\n",
        "Where:\n",
        "\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  = probability of class\n",
        "𝑖\n",
        "i\n",
        "\n",
        "🔍 Interpretation:\n",
        "Entropy = 0 → Pure node\n",
        "\n",
        "Entropy = 1 (max) → Completely mixed classes (in binary classification)\n",
        "\n",
        "🔁 Information Gain:\n",
        "When splitting a node, the Information Gain is calculated as:\n",
        "\n",
        "Information Gain\n",
        "=\n",
        "Entropy (parent)\n",
        "−\n",
        "Weighted Entropy (children)\n",
        "Information Gain=Entropy (parent)−Weighted Entropy (children)\n",
        "📊 Impact on Decision Tree Splits\n",
        "Measure\tUsed For\tObjective\n",
        "Gini Impurity\tCART (Classification Tree)\tMinimizes impurity after split\n",
        "Entropy\tID3, C4.5 Algorithms\tMaximizes information gain"
      ],
      "metadata": {
        "id": "ep6ZijbNqUga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "\n",
        "| Feature            | Pre-Pruning                      | Post-Pruning                         |\n",
        "| ------------------ | -------------------------------- | ------------------------------------ |\n",
        "| Timing             | During tree building             | After tree is fully grown            |\n",
        "| Goal               | Avoid unnecessary splits         | Remove overfitted branches           |\n",
        "| Control Parameters | `max_depth`, `min_samples_split` | Validation performance, pruning cost |\n",
        "| Speed              | Faster                           | Slightly slower                      |\n",
        "| Accuracy           | Might underfit                   | Better generalization                |\n"
      ],
      "metadata": {
        "id": "TkomuZFfqUcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split? *\n",
        "- Information Gain is a metric used to measure the effectiveness of an attribute (feature) in classifying the training data. It quantifies the reduction in entropy (uncertainty or impurity) achieved after a dataset is split based on an attribute.\n",
        "\n",
        "🔢 Formula:\n",
        "Information Gain\n",
        "=\n",
        "Entropy (Parent)\n",
        "−\n",
        "Weighted Entropy (Children)\n",
        "Information Gain=Entropy (Parent)−Weighted Entropy (Children)\n",
        "Where:\n",
        "\n",
        "Entropy measures the disorder/impurity in the dataset.\n",
        "\n",
        "Weighted Entropy = sum of the entropy of each child node, weighted by the proportion of instances.\n",
        "\n",
        "📊 Why is Information Gain Important?\n",
        "At each node, the decision tree needs to choose which feature to split on.\n",
        "\n",
        "Information Gain helps identify the feature that gives the highest reduction in impurity.\n",
        "\n",
        "A higher Information Gain means the feature does a better job at classifying the data."
      ],
      "metadata": {
        "id": "rIn3CVxDqUYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "\n",
        "✅ Advantages of Decision Trees:\n",
        "Feature\tBenefit\n",
        "Easy to Understand\tVisual and logical flow, even non-experts can interpret it\n",
        "Handles Both Data Types\tWorks with both categorical and numerical data\n",
        "Requires Little Prep\tNo need for feature scaling or normalization\n",
        "Fast Prediction\tOnce built, predictions are quick and require minimal computation\n",
        "Feature Importance\tHelps in identifying the most influential variables\n",
        "\n",
        "⚠️ Limitations of Decision Trees:\n",
        "Limitation\tImpact\n",
        "Overfitting\tDeep trees may fit training data too closely and perform poorly on new data\n",
        "Instability\tSmall data changes can lead to a completely different tree structure\n",
        "Bias toward dominant classes\tImbalanced datasets can affect performance\n",
        "Greedy nature\tMakes local optimal decisions that may not be globally optimal\n",
        "Less accurate alone\tOften outperformed by ensemble methods like Random Forest or XGBoost"
      ],
      "metadata": {
        "id": "HGjnuw12qUR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Iris Dataset"
      ],
      "metadata": {
        "id": "5_43oph1qUJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n"
      ],
      "metadata": {
        "id": "NGgzcosKuTin"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQz9po0xu-l7",
        "outputId": "215d862c-543e-453d-93e0-49a9753d7ae6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
              "        [4.9, 3. , 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.3, 0.2],\n",
              "        [4.6, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.6, 1.4, 0.2],\n",
              "        [5.4, 3.9, 1.7, 0.4],\n",
              "        [4.6, 3.4, 1.4, 0.3],\n",
              "        [5. , 3.4, 1.5, 0.2],\n",
              "        [4.4, 2.9, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.1],\n",
              "        [5.4, 3.7, 1.5, 0.2],\n",
              "        [4.8, 3.4, 1.6, 0.2],\n",
              "        [4.8, 3. , 1.4, 0.1],\n",
              "        [4.3, 3. , 1.1, 0.1],\n",
              "        [5.8, 4. , 1.2, 0.2],\n",
              "        [5.7, 4.4, 1.5, 0.4],\n",
              "        [5.4, 3.9, 1.3, 0.4],\n",
              "        [5.1, 3.5, 1.4, 0.3],\n",
              "        [5.7, 3.8, 1.7, 0.3],\n",
              "        [5.1, 3.8, 1.5, 0.3],\n",
              "        [5.4, 3.4, 1.7, 0.2],\n",
              "        [5.1, 3.7, 1.5, 0.4],\n",
              "        [4.6, 3.6, 1. , 0.2],\n",
              "        [5.1, 3.3, 1.7, 0.5],\n",
              "        [4.8, 3.4, 1.9, 0.2],\n",
              "        [5. , 3. , 1.6, 0.2],\n",
              "        [5. , 3.4, 1.6, 0.4],\n",
              "        [5.2, 3.5, 1.5, 0.2],\n",
              "        [5.2, 3.4, 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.6, 0.2],\n",
              "        [4.8, 3.1, 1.6, 0.2],\n",
              "        [5.4, 3.4, 1.5, 0.4],\n",
              "        [5.2, 4.1, 1.5, 0.1],\n",
              "        [5.5, 4.2, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.2, 1.2, 0.2],\n",
              "        [5.5, 3.5, 1.3, 0.2],\n",
              "        [4.9, 3.6, 1.4, 0.1],\n",
              "        [4.4, 3. , 1.3, 0.2],\n",
              "        [5.1, 3.4, 1.5, 0.2],\n",
              "        [5. , 3.5, 1.3, 0.3],\n",
              "        [4.5, 2.3, 1.3, 0.3],\n",
              "        [4.4, 3.2, 1.3, 0.2],\n",
              "        [5. , 3.5, 1.6, 0.6],\n",
              "        [5.1, 3.8, 1.9, 0.4],\n",
              "        [4.8, 3. , 1.4, 0.3],\n",
              "        [5.1, 3.8, 1.6, 0.2],\n",
              "        [4.6, 3.2, 1.4, 0.2],\n",
              "        [5.3, 3.7, 1.5, 0.2],\n",
              "        [5. , 3.3, 1.4, 0.2],\n",
              "        [7. , 3.2, 4.7, 1.4],\n",
              "        [6.4, 3.2, 4.5, 1.5],\n",
              "        [6.9, 3.1, 4.9, 1.5],\n",
              "        [5.5, 2.3, 4. , 1.3],\n",
              "        [6.5, 2.8, 4.6, 1.5],\n",
              "        [5.7, 2.8, 4.5, 1.3],\n",
              "        [6.3, 3.3, 4.7, 1.6],\n",
              "        [4.9, 2.4, 3.3, 1. ],\n",
              "        [6.6, 2.9, 4.6, 1.3],\n",
              "        [5.2, 2.7, 3.9, 1.4],\n",
              "        [5. , 2. , 3.5, 1. ],\n",
              "        [5.9, 3. , 4.2, 1.5],\n",
              "        [6. , 2.2, 4. , 1. ],\n",
              "        [6.1, 2.9, 4.7, 1.4],\n",
              "        [5.6, 2.9, 3.6, 1.3],\n",
              "        [6.7, 3.1, 4.4, 1.4],\n",
              "        [5.6, 3. , 4.5, 1.5],\n",
              "        [5.8, 2.7, 4.1, 1. ],\n",
              "        [6.2, 2.2, 4.5, 1.5],\n",
              "        [5.6, 2.5, 3.9, 1.1],\n",
              "        [5.9, 3.2, 4.8, 1.8],\n",
              "        [6.1, 2.8, 4. , 1.3],\n",
              "        [6.3, 2.5, 4.9, 1.5],\n",
              "        [6.1, 2.8, 4.7, 1.2],\n",
              "        [6.4, 2.9, 4.3, 1.3],\n",
              "        [6.6, 3. , 4.4, 1.4],\n",
              "        [6.8, 2.8, 4.8, 1.4],\n",
              "        [6.7, 3. , 5. , 1.7],\n",
              "        [6. , 2.9, 4.5, 1.5],\n",
              "        [5.7, 2.6, 3.5, 1. ],\n",
              "        [5.5, 2.4, 3.8, 1.1],\n",
              "        [5.5, 2.4, 3.7, 1. ],\n",
              "        [5.8, 2.7, 3.9, 1.2],\n",
              "        [6. , 2.7, 5.1, 1.6],\n",
              "        [5.4, 3. , 4.5, 1.5],\n",
              "        [6. , 3.4, 4.5, 1.6],\n",
              "        [6.7, 3.1, 4.7, 1.5],\n",
              "        [6.3, 2.3, 4.4, 1.3],\n",
              "        [5.6, 3. , 4.1, 1.3],\n",
              "        [5.5, 2.5, 4. , 1.3],\n",
              "        [5.5, 2.6, 4.4, 1.2],\n",
              "        [6.1, 3. , 4.6, 1.4],\n",
              "        [5.8, 2.6, 4. , 1.2],\n",
              "        [5. , 2.3, 3.3, 1. ],\n",
              "        [5.6, 2.7, 4.2, 1.3],\n",
              "        [5.7, 3. , 4.2, 1.2],\n",
              "        [5.7, 2.9, 4.2, 1.3],\n",
              "        [6.2, 2.9, 4.3, 1.3],\n",
              "        [5.1, 2.5, 3. , 1.1],\n",
              "        [5.7, 2.8, 4.1, 1.3],\n",
              "        [6.3, 3.3, 6. , 2.5],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [7.1, 3. , 5.9, 2.1],\n",
              "        [6.3, 2.9, 5.6, 1.8],\n",
              "        [6.5, 3. , 5.8, 2.2],\n",
              "        [7.6, 3. , 6.6, 2.1],\n",
              "        [4.9, 2.5, 4.5, 1.7],\n",
              "        [7.3, 2.9, 6.3, 1.8],\n",
              "        [6.7, 2.5, 5.8, 1.8],\n",
              "        [7.2, 3.6, 6.1, 2.5],\n",
              "        [6.5, 3.2, 5.1, 2. ],\n",
              "        [6.4, 2.7, 5.3, 1.9],\n",
              "        [6.8, 3. , 5.5, 2.1],\n",
              "        [5.7, 2.5, 5. , 2. ],\n",
              "        [5.8, 2.8, 5.1, 2.4],\n",
              "        [6.4, 3.2, 5.3, 2.3],\n",
              "        [6.5, 3. , 5.5, 1.8],\n",
              "        [7.7, 3.8, 6.7, 2.2],\n",
              "        [7.7, 2.6, 6.9, 2.3],\n",
              "        [6. , 2.2, 5. , 1.5],\n",
              "        [6.9, 3.2, 5.7, 2.3],\n",
              "        [5.6, 2.8, 4.9, 2. ],\n",
              "        [7.7, 2.8, 6.7, 2. ],\n",
              "        [6.3, 2.7, 4.9, 1.8],\n",
              "        [6.7, 3.3, 5.7, 2.1],\n",
              "        [7.2, 3.2, 6. , 1.8],\n",
              "        [6.2, 2.8, 4.8, 1.8],\n",
              "        [6.1, 3. , 4.9, 1.8],\n",
              "        [6.4, 2.8, 5.6, 2.1],\n",
              "        [7.2, 3. , 5.8, 1.6],\n",
              "        [7.4, 2.8, 6.1, 1.9],\n",
              "        [7.9, 3.8, 6.4, 2. ],\n",
              "        [6.4, 2.8, 5.6, 2.2],\n",
              "        [6.3, 2.8, 5.1, 1.5],\n",
              "        [6.1, 2.6, 5.6, 1.4],\n",
              "        [7.7, 3. , 6.1, 2.3],\n",
              "        [6.3, 3.4, 5.6, 2.4],\n",
              "        [6.4, 3.1, 5.5, 1.8],\n",
              "        [6. , 3. , 4.8, 1.8],\n",
              "        [6.9, 3.1, 5.4, 2.1],\n",
              "        [6.7, 3.1, 5.6, 2.4],\n",
              "        [6.9, 3.1, 5.1, 2.3],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [6.8, 3.2, 5.9, 2.3],\n",
              "        [6.7, 3.3, 5.7, 2.5],\n",
              "        [6.7, 3. , 5.2, 2.3],\n",
              "        [6.3, 2.5, 5. , 1.9],\n",
              "        [6.5, 3. , 5.2, 2. ],\n",
              "        [6.2, 3.4, 5.4, 2.3],\n",
              "        [5.9, 3. , 5.1, 1.8]]),\n",
              " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " 'frame': None,\n",
              " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
              " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. dropdown:: References\\n\\n  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n    Mathematical Statistics\" (John Wiley, NY, 1950).\\n  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n    Structure and Classification Rule for Recognition in Partially Exposed\\n    Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n    on Information Theory, May 1972, 431-433.\\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n    conceptual clustering system finds 3 classes in the data.\\n  - Many, many more ...\\n',\n",
              " 'feature_names': ['sepal length (cm)',\n",
              "  'sepal width (cm)',\n",
              "  'petal length (cm)',\n",
              "  'petal width (cm)'],\n",
              " 'filename': 'iris.csv',\n",
              " 'data_module': 'sklearn.datasets.data'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xeJ4eSHvB2_",
        "outputId": "67c87147-093e-47b7-c392-513969048e77"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
              "          37.88      , -122.23      ],\n",
              "       [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
              "          37.86      , -122.22      ],\n",
              "       [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
              "          37.85      , -122.24      ],\n",
              "       ...,\n",
              "       [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
              "          39.43      , -121.22      ],\n",
              "       [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
              "          39.43      , -121.32      ],\n",
              "       [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
              "          39.37      , -121.24      ]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaCwSJ7yvJOW",
        "outputId": "5c0de0b8-3113-49e1-821b-f527bae97121"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boston Housing Dataset"
      ],
      "metadata": {
        "id": "jysCZMPOqT_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F_mjF-zIqPU8"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3Pg5IThu9b5",
        "outputId": "4f61d724-acff-4574-f53a-2f7538de7286"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
              "          37.88      , -122.23      ],\n",
              "       [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
              "          37.86      , -122.22      ],\n",
              "       [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
              "          37.85      , -122.24      ],\n",
              "       ...,\n",
              "       [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
              "          39.43      , -121.22      ],\n",
              "       [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
              "          39.43      , -121.32      ],\n",
              "       [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
              "          39.37      , -121.24      ]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMj515vPvP8l",
        "outputId": "ac74f7db-9789-493b-8995-d49f8dc94f99"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier using the Gini criterion\n",
        "# ● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "xvCe-EAXvV5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display feature importances\n",
        "feature_importances = pd.Series(clf.feature_importances_, index=feature_names)\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances.sort_values(ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLCsqlgZvSBl",
        "outputId": "783c5381-cbdf-4f58-d194-ccdac909f882"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "\n",
            "Feature Importances:\n",
            "petal length (cm)    0.906143\n",
            "petal width (cm)     0.077186\n",
            "sepal width (cm)     0.016670\n",
            "sepal length (cm)    0.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "# a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "nAuFPoGAv-il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model 1: Decision Tree with max_depth = 3 (shallow tree)\n",
        "clf_shallow = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_shallow.fit(X_train, y_train)\n",
        "y_pred_shallow = clf_shallow.predict(X_test)\n",
        "acc_shallow = accuracy_score(y_test, y_pred_shallow)\n",
        "\n",
        "# Model 2: Fully grown Decision Tree (no depth limit)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print comparison\n",
        "print(f\"Shallow Tree Accuracy (max_depth=3): {acc_shallow:.2f}\")\n",
        "print(f\"Fully Grown Tree Accuracy           : {acc_full:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFjmwOs8vwI6",
        "outputId": "d4244332-e694-4a97-91b9-f662acc2ab1a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shallow Tree Accuracy (max_depth=3): 1.00\n",
            "Fully Grown Tree Accuracy           : 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# ● Load the Boston Housing Dataset\n",
        "# ● Train a Decision Tree Regressor\n",
        "# ● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "fbGBMKv5wIir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "# Display feature importances\n",
        "importances = pd.Series(regressor.feature_importances_, index=feature_names)\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(importances.sort_values(ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MiMyCb1wG2M",
        "outputId": "f286943d-67e1-4f3d-9157-6c44cf28279a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.50\n",
            "\n",
            "Feature Importances:\n",
            "MedInc        0.528509\n",
            "AveOccup      0.130838\n",
            "Latitude      0.093717\n",
            "Longitude     0.082902\n",
            "AveRooms      0.052975\n",
            "HouseAge      0.051884\n",
            "Population    0.030516\n",
            "AveBedrms     0.028660\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "# ● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "NRs0yVd9wbu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid to tune\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate on test data\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Best Parameters: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruRxFAWWwX7J",
        "outputId": "a06bd371-d675-4c07-e631-460e15a503d6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy with Best Parameters: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "# Explain the step-by-step process you would follow to:\n",
        "\n",
        "# ● Handle the missing values\n",
        "\n",
        "# ● Encode the categorical features\n",
        "\n",
        "# ● Train a Decision Tree model\n",
        "\n",
        "# ● Tune its hyperparameters\n",
        "\n",
        "# ● Evaluate its performance\n",
        "\n",
        "# And describe what business value this model could provide in the real-world setting\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- STEP-BY-STEP PROCESS\n",
        "\n",
        "1️⃣ **Handle Missing Values**\n",
        "\n",
        "🔍 Explore missing data:\n",
        "\n",
        "df.isnull().sum()\n",
        "\n",
        "✅ Handling strategies:\n",
        "\n",
        "Numerical features: Impute using mean or median:\n",
        "\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer_num = SimpleImputer(strategy='median')\n",
        "\n",
        "\n",
        "df[numerical_cols] = imputer_num.fit_transform(df[numerical_cols])\n",
        "\n",
        "**Categorical features**: Impute using most frequent:\n",
        "\n",
        "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])\n",
        "\n",
        "2️⃣ Encode Categorical Features\n",
        "\n",
        "✅ Use One-Hot Encoding for nominal features (e.g., gender, region):\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "df = pd.get_dummies(df, columns=categorical_cols)\n",
        "\n",
        "✅ Use Label Encoding if feature is ordinal (e.g., mild < moderate < severe):\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "df['severity'] = le.fit_transform(df['severity'])\n",
        "\n",
        "3️⃣ Train the Decision Tree Model\n",
        "\n",
        "✅ Split the data:\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "✅ Train the model:\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "4️⃣ Tune Hyperparameters\n",
        "\n",
        "\n",
        "✅ Use GridSearchCV to find best parameters:\n",
        "\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "5️⃣** Evaluate Model Performance**\n",
        "✅ Use common metrics:\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "📈  Visualize the decision tree:\n",
        "\n",
        "\n",
        "from sklearn.tree import plot_tree\n",
        "plot_tree(best_model, filled=True, feature_names=X.columns, class_names=['No Disease', 'Disease'"
      ],
      "metadata": {
        "id": "hVDRMinFwpZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " ############################END##########################################"
      ],
      "metadata": {
        "id": "qL29I_t1zF-O"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F9P-6KhM5Tvm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}